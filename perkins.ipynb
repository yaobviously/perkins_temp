{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perkins.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_QJ13QQxNaNbQQvIaFWLy20jvrs4MgWM",
      "authorship_tag": "ABX9TyNW6f2/u95ulx+MiZGOpwno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaobviously/perkins_temp/blob/main/perkins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXsjTbsgPwta"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dll-fa2CP0cO"
      },
      "source": [
        "pop_csv = '/content/drive/MyDrive/Temperature_Perkins/Population Data.csv'\n",
        "temp_csv = '/content/drive/MyDrive/Temperature_Perkins/Temperature Data.csv'"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMwBV_2BP2uy"
      },
      "source": [
        "def load_transform_data():\n",
        "  \"\"\"\n",
        "  Preparing the temperature and population datasets for visualization and \n",
        "  analysis\n",
        "  \"\"\"\n",
        "\n",
        "  temp_df = pd.read_csv(temp_csv, parse_dates=['location_date'])\n",
        "\n",
        "  pop_df = pd.read_csv(pop_csv)\n",
        "\n",
        "  # case normalizing pop columns\n",
        "  pop_df.columns = pop_df.columns.str.lower()\n",
        "\n",
        "  # dropping redundant columns\n",
        "  col_drop = ['country_name', 'country_code', 'continent']\n",
        "  temp_df = temp_df.drop(col_drop, axis=1)\n",
        "\n",
        "  # renaming columns - celsius will be indicated in viz\n",
        "  temp_df = temp_df.rename(columns={'temp_mean_c': 'mean_temp',\n",
        "                                    'temp_min_c' : 'min_temp',\n",
        "                                    'temp_max_c' : 'max_temp'})\n",
        "\n",
        "  # making it easier to merge with pop df\n",
        "  temp_df['name'] = [x.split('/')[0] for x in temp_df['name']]\n",
        "  temp_df = temp_df.rename(columns={'name':'city'})\n",
        "\n",
        "  # creating a city dictionary to find matching city pops. i'm replacing \n",
        "  # Albany with Syracuse because it seems relatively close in terms of pop (for weighting)\n",
        "  # transparent hack - could easily hard code the population using google.com, \n",
        "  # but i'm using ONLY dataset provided + rough knowledge of Albany pop.\n",
        "\n",
        "  city_dict = {'St Louis' : 'St. Louis', 'NYC' : 'New York', \"Chicago O'Hare\" : 'Chicago',\n",
        "               'Covington': 'Cincinnati', 'Wash DC' : 'Washington', 'Windsor Locks' : 'Hartford',\n",
        "               'Albany' : 'Syracuse'\n",
        "               }\n",
        "  temp_df['city'] = temp_df['city'].map(city_dict).fillna(temp_df['city'])\n",
        "\n",
        "  reverse_dict = {v:k for k,v in city_dict.items()}\n",
        "  \n",
        "  # merging the dataframes and fixing the Albany hack above\n",
        "\n",
        "  merged_df = pd.merge(temp_df, pop_df, how='left', on='city')\n",
        "  merged_df = merged_df.set_index('location_date').sort_index()\n",
        "  \n",
        "  merged_df['city'] = merged_df['city'].map(reverse_dict).fillna(merged_df['city'])\n",
        "  merged_df['city'] = merged_df['city'].replace('Wash DC', 'Washington (Dulles)')\n",
        "  # creating a function to find the dates with missing values for each\n",
        "  # station and then appending all of those dates to the main dataframe\n",
        "\n",
        "  days = merged_df.index.unique()\n",
        "\n",
        "  def find_miss_days(station_code = 'KLIT'):\n",
        "\n",
        "    df_ = merged_df[merged_df['station_code'] == station_code]\n",
        "\n",
        "    indexes = set(df_.index)\n",
        "    dates_missing = set(days).difference(indexes)\n",
        "\n",
        "    val_dict = {'city' : df_['city'][0],\n",
        "                'station_code' : station_code,\n",
        "                'state' : df_['state'][0],\n",
        "                'population' : df_['population'][0],\n",
        "                'lon' : df_['lon'][0],\n",
        "                'lat' : df_['lat'][0]\n",
        "                }\n",
        "\n",
        "    new_df = pd.DataFrame(val_dict, index=tuple(dates_missing))\n",
        "\n",
        "    return new_df\n",
        "\n",
        "  # running the above function to create dfs to add to main df\n",
        "  df_append = [find_miss_days(code) for code in merged_df.station_code.unique()]\n",
        "  df_concat = pd.concat(df_append)\n",
        "\n",
        "  df = merged_df.append(df_concat)\n",
        "\n",
        "  # sorting the index to get the new rows where they belong\n",
        "  df = df.sort_index()\n",
        "\n",
        "  # creatin a multilevel index to ensure unique index values for all rows\n",
        "  df = df.set_index('station_code', append=True)\n",
        "  \n",
        "  # extracting month/year from index to compute averages for each \n",
        "  # location\n",
        "  df['month'] = pd.DatetimeIndex(df.index.get_level_values(0)).month\n",
        "  df['year'] = pd.DatetimeIndex(df.index.get_level_values(0)).year\n",
        "\n",
        "  # computing rolling averages for the purpose of imputation. for predictive \n",
        "  # modelling i would utilize shift()\n",
        "  \n",
        "  df['5day_rolling_mean'] = df.groupby(df.index.get_level_values(1))['mean_temp'].transform(lambda x: x.rolling(5, min_periods=3).mean().fillna(method='ffill').fillna(method='bfill'))\n",
        "  df['5day_rolling_max'] = df.groupby(df.index.get_level_values(1))['max_temp'].transform(lambda x: x.rolling(5, min_periods=3).mean().fillna(method='ffill').fillna(method='bfill'))\n",
        "  df['5day_rolling_min'] = df.groupby(df.index.get_level_values(1))['min_temp'].transform(lambda x: x.rolling(5, min_periods=3).mean().fillna(method='ffill').fillna(method='bfill'))\n",
        "  \n",
        "  # computing 30 and 90 day rolling means - doesn't appear to be necessary but i'll leave it\n",
        "  df['30day_rolling_mean'] = df.groupby(df.index.get_level_values(1))['mean_temp'].transform(lambda x: x.rolling(30, min_periods=5).mean().fillna(method='ffill').fillna(method='bfill'))\n",
        "  df['90day_rolling_mean'] = df.groupby(df.index.get_level_values(1))['mean_temp'].transform(lambda x: x.rolling(90, min_periods=5).mean().fillna(method='ffill').fillna(method='bfill'))\n",
        "  \n",
        "  # computing cumulative expanding averages for this dataset\n",
        "  df['monthly_average_average'] = df.groupby([df.index.get_level_values(1), 'month'])['mean_temp'].transform(lambda x: x.expanding().mean().fillna(method='bfill'))\n",
        "  df['monthly_max_average'] = df.groupby([df.index.get_level_values(1), 'month'])['max_temp'].transform(lambda x: x.expanding().mean().fillna(method='bfill'))\n",
        "  df['monthly_min_average'] = df.groupby([df.index.get_level_values(1), 'month'])['min_temp'].transform(lambda x: x.expanding().mean().fillna(method='bfill'))\n",
        "\n",
        "  # computing the *specific* monthly average, high, and low\n",
        "  df['this_month_average'] = df.groupby([df.index.get_level_values(1), 'year', 'month'])['mean_temp'].transform(lambda x: x.expanding().mean().fillna(method='bfill'))\n",
        "  df['this_month_high'] = df.groupby([df.index.get_level_values(1), 'year', 'month'])['max_temp'].transform(lambda x: x.expanding().max().fillna(method='bfill'))\n",
        "  df['this_month_low'] = df.groupby([df.index.get_level_values(1), 'year', 'month'])['mean_temp'].transform(lambda x: x.expanding().min().fillna(method='bfill'))\n",
        "  \n",
        "  # flagging rows that will have imputed daily temperature values\n",
        "  df['imputed_mean'] = [1 if pd.isna(mean) else 0 for mean in df['mean_temp']]\n",
        "  \n",
        "  # filling in missing values with rolling average\n",
        "  df['mean_temp'] = [y if pd.isna(x) else x for x, y in zip(df['mean_temp'], df['5day_rolling_mean'])]\n",
        "  df['max_temp'] = [y if pd.isna(x) else x for x, y in zip(df['max_temp'], df['5day_rolling_max'])]\n",
        "  df['min_temp'] = [y if pd.isna(x) else x for x, y in zip(df['min_temp'], df['5day_rolling_min'])]  \n",
        "  \n",
        "  # rounding and fixing columns\n",
        "  df['city_full'] = [\", \".join(x) for x in zip(df['city'], df['state'])]\n",
        "  drop_cols = ['city', 'state']\n",
        "  df = df.drop(drop_cols, axis=1)\n",
        "  df = df.round(2)  \n",
        "  \n",
        "  return df\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yUvEfmBdjWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60563d64-016d-433b-ad38-4348dfb504a0"
      },
      "source": [
        "df = load_transform_data()\n",
        "\n",
        "print(\"The shape of the new dataframe is:\", df.shape)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the new dataframe is: (94353, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKUlhtSoWfW4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "43e9cd31-e52e-491c-a998-0cf509477f0b"
      },
      "source": [
        "def plot_city(city = 'Sacramento', duration='W'):\n",
        "  \"\"\"\n",
        "  A simple plotting function that takes a city and a duration and displays a graph\n",
        "  of the average of the mean temp, max temp, and min temp over that time span\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "    city : str\n",
        "    duration: str (\"D\", \"W\", \"M\", \"A\")\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "    plot: matplot plot\n",
        "  \"\"\"\n",
        "\n",
        "  cols = ['mean_temp', 'max_temp', 'min_temp']\n",
        "\n",
        "  df_ = df[df['city'] == city]\n",
        "\n",
        "  fig, ax = \n",
        "\n",
        "  return df_[cols].resample(duration).mean().plot(figsize=(24,5), grid=True, legend=True);"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-f775dd77d14c>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    fig, ax =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4pfkzNQSxhR"
      },
      "source": [
        "def estimate_pop(date = '2020-05-01', temp=25):\n",
        "  \"\"\"\n",
        "  Takes a date and a temperature and tells you how many residents in the \n",
        "  dataset experienced temperatures above the entered value on that date\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "    date: datetime\n",
        "    temp: int\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "    total population: float\n",
        "  \"\"\"\n",
        "\n",
        "  df_ = df[(df.index == date) & (df['max_temp'] >= temp)]\n",
        "  return df_.population.sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qJOR4wsZcDp",
        "outputId": "fea3982c-168d-4ece-b605-1f4748bc6da3"
      },
      "source": [
        "print(f\"On March 22nd, 2018 {estimate_pop(date='2018-03-22', temp=21)} experienced temperatures above 21C\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On March 22nd, 2018 5818554.0 experienced temperatures above 21C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}